{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y91tViSBLV1H","outputId":"442b7eec-098e-4cd7-9318-0b617776f3ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"y91tViSBLV1H"},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhXetreeLec3"},"outputs":[],"source":["# import zipfile\n","# import shutil\n","\n","# %cd /content/drive/MyDrive/2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation/open\n","\n","# !unzip -qq \"/content/drive/MyDrive/2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation/open.zip\""],"id":"IhXetreeLec3"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gd9PB8sezC9X","outputId":"cae0428d-0a0e-4c3f-a919-b5a4f6b63488"},"outputs":[{"output_type":"stream","name":"stdout","text":["test_image : 1898\n","train_source_gt : 2194\n","train_source_image : 2194\n","train_target_image : 2923\n","val_source_gt : 466\n","val_source_image : 466\n"]}],"source":["import os\n","\n","test_image_path = \"/content/drive/MyDrive/2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation/open/test_image\"\n","train_source_gt_path = \"/content/drive/MyDrive/2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation/open/train_source_gt\"\n","train_source_image_path = \"/content/drive/MyDrive/2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation/open/train_source_image\"\n","train_target_image_path = \"/content/drive/MyDrive/2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation/open/train_target_image\"\n","val_source_gt_path = \"/content/drive/MyDrive/2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation/open/val_source_gt\"\n","val_source_image_path = \"/content/drive/MyDrive/2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation/open/val_source_image\"\n","\n","file_list1 = os.listdir(test_image_path)\n","file_list2 = os.listdir(train_source_gt_path)\n","file_list3 = os.listdir(train_source_image_path)\n","file_list4 = os.listdir(train_target_image_path)\n","file_list5 = os.listdir(val_source_gt_path)\n","file_list6 = os.listdir(val_source_image_path)\n","\n","file_count1 = len(file_list1)\n","file_count2 = len(file_list2)\n","file_count3 = len(file_list3)\n","file_count4 = len(file_list4)\n","file_count5 = len(file_list5)\n","file_count6 = len(file_list6)\n","\n","print('test_image :', file_count1)\n","print('train_source_gt :', file_count2)\n","print('train_source_image :', file_count3)\n","print('train_target_image :', file_count4)\n","print('val_source_gt :', file_count5)\n","print('val_source_image :', file_count6)\n","\n","# test_image         : 1898\n","# train_source_gt    : 2194\n","# train_source_image : 2194\n","# train_target_image : 2923\n","# val_source_gt      : 466\n","# val_source_image   : 466"],"id":"Gd9PB8sezC9X"},{"cell_type":"markdown","metadata":{"id":"d73d24e3-5c9e-4ade-9e6e-ca6f46a2d914"},"source":["## Import"],"id":"d73d24e3-5c9e-4ade-9e6e-ca6f46a2d914"},{"cell_type":"code","source":["!pip install efficientnet-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"heHB6e-2ZgTw","outputId":"1447debe-3195-4c79-aad3-6e2157982083"},"id":"heHB6e-2ZgTw","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting efficientnet-pytorch\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\n","Building wheels for collected packages: efficientnet-pytorch\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=e055e17591b1a1c1db7b89e7820153f8454f675d5fea562a6bba9209852d9407\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","Successfully built efficientnet-pytorch\n","Installing collected packages: efficientnet-pytorch\n","Successfully installed efficientnet-pytorch-0.7.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad9b681e-370a-4cfa-a452-dd2d7f0cd77f"},"outputs":[],"source":["import cv2\n","from PIL import Image\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from tqdm import tqdm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from torch.optim import lr_scheduler\n","import torchvision.models.detection.mask_rcnn\n","import torchvision.models as models\n","from efficientnet_pytorch import EfficientNet\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"id":"ad9b681e-370a-4cfa-a452-dd2d7f0cd77f"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"57d9c552-8a08-44b9-d538-bd1c5ef9cb25","id":"wmBCPn1Wa2vR"},"outputs":[{"output_type":"stream","name":"stdout","text":["Current working directory: /content/drive/MyDrive/2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation/open\n","New current working directory: /content/drive/MyDrive/2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation/open\n"]}],"source":["# 현재 작업 디렉토리 확인\n","current_directory = os.getcwd()\n","print(\"Current working directory:\", current_directory)\n","\n","# 새로운 작업 디렉토리로 변경\n","new_directory = \"/content/drive/MyDrive/2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation/open\"  # 새로운 디렉토리 경로를 지정\n","os.chdir(new_directory)\n","\n","# 변경된 현재 작업 디렉토리 확인\n","current_directory = os.getcwd()\n","print(\"New current working directory:\", current_directory)"],"id":"wmBCPn1Wa2vR"},{"cell_type":"markdown","metadata":{"id":"20ff3de5-0d0e-497b-ac75-d5179a3f65d3"},"source":["## Utils"],"id":"20ff3de5-0d0e-497b-ac75-d5179a3f65d3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"838e1d83-8670-407b-82f6-bf9652f58639"},"outputs":[],"source":["# RLE 인코딩 함수\n","def rle_encode(mask):\n","    pixels = mask.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"],"id":"838e1d83-8670-407b-82f6-bf9652f58639"},{"cell_type":"markdown","metadata":{"id":"be76a29e-e9c2-411a-a569-04166f074184"},"source":["## Custom Dataset"],"id":"be76a29e-e9c2-411a-a569-04166f074184"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8496767-2f64-4285-bec4-c6f53a1fd9d2"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.infer = infer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data.iloc[idx, 1]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.infer:\n","            if self.transform:\n","                image = self.transform(image=image)['image']\n","            return image\n","\n","        mask_path = self.data.iloc[idx, 2]\n","        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","        mask[mask == 255] = 12 #배경을 픽셀값 12로 간주\n","\n","        if self.transform:\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        return image, mask"],"id":"a8496767-2f64-4285-bec4-c6f53a1fd9d2"},{"cell_type":"code","source":["# class CustomDataset(Dataset):\n","#     def __init__(self, csv_file, transform=None, infer=False):\n","#         self.data = pd.read_csv(csv_file)\n","#         self.transform = transform\n","#         self.infer = infer\n","\n","#     def __len__(self):\n","#         return len(self.data)\n","\n","#     def __getitem__(self, idx):\n","#         img_path = self.data.iloc[idx, 1]\n","#         image = cv2.imread(img_path)\n","#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","#         if self.infer:\n","#             # 가우시안 필터를 적용합니다.\n","#             image = cv2.GaussianBlur(image, (5, 5), 0)  # (5, 5)는 커널 크기, 0은 표준 편차입니다.\n","\n","#             if self.transform:\n","#                 image = self.transform(image=image)['image']\n","#             return image\n","\n","#         mask_path = self.data.iloc[idx, 2]\n","#         mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","#         mask[mask == 255] = 12  # 배경을 픽셀값 12로 간주\n","\n","#         if self.transform:\n","#             augmented = self.transform(image=image, mask=mask)\n","#             image = augmented['image']\n","#             mask = augmented['mask']\n","\n","#         return image, mask"],"metadata":{"id":"9khggSK4VMVm"},"id":"9khggSK4VMVm","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dc955893-22fd-4320-88be-7aa0d790cbd9"},"source":["## Data Loader"],"id":"dc955893-22fd-4320-88be-7aa0d790cbd9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1b708503-2ff9-4584-9d73-40990b3572f8"},"outputs":[],"source":["transform = A.Compose(\n","    [\n","        A.Resize(224, 224),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ]\n",")\n","\n","dataset = CustomDataset(csv_file='./train_source.csv', transform=transform)\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n","\n","val_dataset = CustomDataset(csv_file='./val_source.csv', transform=transform)\n","val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=4)"],"id":"1b708503-2ff9-4584-9d73-40990b3572f8"},{"cell_type":"markdown","metadata":{"id":"f42501fc-b573-4893-a7c4-5e280dfdaf09"},"source":["## Define Model"],"id":"f42501fc-b573-4893-a7c4-5e280dfdaf09"},{"cell_type":"code","execution_count":null,"metadata":{"id":"65960bfb-803a-4c40-b713-6f647779e4ea"},"outputs":[],"source":["# U-Net의 기본 구성 요소인 Double Convolution Block을 정의합니다.\n","def double_conv(in_channels, out_channels):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True)\n","    )\n","\n","# 간단한 U-Net 모델 정의\n","class UNet(nn.Module):\n","    def __init__(self):\n","        super(UNet, self).__init__()\n","        self.dconv_down1 = double_conv(3, 64)\n","        self.dconv_down2 = double_conv(64, 128)\n","        self.dconv_down3 = double_conv(128, 256)\n","        self.dconv_down4 = double_conv(256, 512)\n","\n","        self.maxpool = nn.MaxPool2d(2)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","\n","        self.dconv_up3 = double_conv(256 + 512, 256)\n","        self.dconv_up2 = double_conv(128 + 256, 128)\n","        self.dconv_up1 = double_conv(128 + 64, 64)\n","\n","        self.conv_last = nn.Conv2d(64, 13, 1) # 12개 class + 1 background\n","\n","    def forward(self, x):\n","        conv1 = self.dconv_down1(x)\n","        x = self.maxpool(conv1)\n","\n","        conv2 = self.dconv_down2(x)\n","        x = self.maxpool(conv2)\n","\n","        conv3 = self.dconv_down3(x)\n","        x = self.maxpool(conv3)\n","\n","        x = self.dconv_down4(x)\n","\n","        x = self.upsample(x)\n","        x = torch.cat([x, conv3], dim=1)\n","\n","        x = self.dconv_up3(x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, conv2], dim=1)\n","\n","        x = self.dconv_up2(x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, conv1], dim=1)\n","\n","        x = self.dconv_up1(x)\n","\n","        out = self.conv_last(x)\n","\n","        return out\n","\n","#\t0.1466746583"],"id":"65960bfb-803a-4c40-b713-6f647779e4ea"},{"cell_type":"code","source":["# SegNet의 인코더 부분 정의\n","class SegNetEncoder(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(SegNetEncoder, self).__init__()\n","        self.encode = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n","        )\n","\n","    def forward(self, x):\n","        x, indices = self.encode(x)\n","        return x, indices\n","\n","# SegNet의 디코더 부분 정의\n","class SegNetDecoder(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(SegNetDecoder, self).__init__()\n","        self.decode = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x, indices):\n","        x = nn.functional.max_unpool2d(x, indices, kernel_size=2, stride=2)\n","        x = self.decode(x)\n","        return x\n","\n","# 모델 아키텍처에서 indices 정보를 저장하도록 수정\n","class SegNet(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(SegNet, self).__init__()\n","        self.encoder1 = SegNetEncoder(in_channels, 64)\n","        self.encoder2 = SegNetEncoder(64, 128)\n","        self.encoder3 = SegNetEncoder(128, 256)\n","        self.encoder4 = SegNetEncoder(256, 512)\n","\n","        self.decoder4 = SegNetDecoder(512, 256)\n","        self.decoder3 = SegNetDecoder(256, 128)\n","        self.decoder2 = SegNetDecoder(128, 64)\n","        self.decoder1 = SegNetDecoder(64, num_classes)\n","\n","    def forward(self, x):\n","        x, indices1 = self.encoder1(x)\n","        x, indices2 = self.encoder2(x)\n","        x, indices3 = self.encoder3(x)\n","        x, indices4 = self.encoder4(x)\n","\n","        x = self.decoder4(x, indices4)\n","        x = self.decoder3(x, indices3)\n","        x = self.decoder2(x, indices2)\n","        x = self.decoder1(x, indices1)\n","\n","        return x\n","\n","# 0.1249705711"],"metadata":{"id":"atIXVDUhaqTi"},"id":"atIXVDUhaqTi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FCN(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(FCN, self).__init__()\n","\n","        # Encoder\n","        self.encoder_conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=3, padding=100),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n","        )\n","\n","        self.encoder_conv2 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n","        )\n","\n","        self.encoder_conv3 = nn.Sequential(\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n","        )\n","\n","        self.encoder_conv4 = nn.Sequential(\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n","        )\n","\n","        self.encoder_conv5 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n","        )\n","\n","        # Fully Convolutional Layers\n","        self.fully_conv1 = nn.Conv2d(512, 4096, kernel_size=7)\n","        self.fully_conv2 = nn.Conv2d(4096, 4096, kernel_size=1)\n","\n","        # Score layers\n","        self.score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n","        self.upscore = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, stride=32, bias=False)\n","\n","    def forward(self, x):\n","        # Encoder\n","        x1 = self.encoder_conv1(x)\n","        x2 = self.encoder_conv2(x1)\n","        x3 = self.encoder_conv3(x2)\n","        x4 = self.encoder_conv4(x3)\n","        x5 = self.encoder_conv5(x4)\n","\n","        # Fully Convolutional Layers\n","        x = self.fully_conv1(x5)\n","        x = self.fully_conv2(x)\n","\n","        # Score layers\n","        x = self.score_fr(x)\n","\n","        # Upsample\n","        x = self.upscore(x)\n","        x = x[:, :, 64:64 + x.size(2), 64:64 + x.size(3)].contiguous()  # Crop the output to match the target size (224x224)\n","\n","        return x\n","\n","# 0.1251070643"],"metadata":{"id":"75euDuWCiB3n"},"id":"75euDuWCiB3n","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class LightweightPSPNet(nn.Module):\n","#     def __init__(self, in_channels, num_classes):\n","#         super(LightweightPSPNet, self).__init__()\n","\n","#         # Lightweight Backbone 네트워크 (예: ResNet-18)\n","#         self.backbone = models.resnet18(pretrained=True)\n","#         self.backbone.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","#         self.backbone.fc = nn.Identity()  # Fully-connected 레이어를 Identity로 대체하여 제거\n","\n","#         # Pyramid Pooling Module 정의\n","#         self.ppm = PSPModule(512, 256, sizes=(1, 2, 3, 6))\n","\n","#         # Classifier\n","#         self.final = nn.Sequential(\n","#             nn.Conv2d(512, num_classes, kernel_size=1)  # ResNet-18의 출력 채널 수는 512\n","#         )\n","\n","#     def forward(self, x):\n","#         # Backbone 네트워크 수행\n","#         x = self.backbone.conv1(x)\n","#         x = self.backbone.bn1(x)\n","#         x = self.backbone.relu(x)\n","#         x = self.backbone.maxpool(x)\n","#         x1 = self.backbone.layer1(x)\n","#         x2 = self.backbone.layer2(x1)\n","#         x3 = self.backbone.layer3(x2)\n","#         x4 = self.backbone.layer4(x3)\n","\n","#         # Pyramid Pooling Module 수행\n","#         x_ppm = self.ppm(x4)\n","\n","#         # Upsampling 및 병합\n","#         x_ppm = F.interpolate(x_ppm, size=x3.size()[2:], mode='bilinear', align_corners=True)  # 크기를 x3와 일치하도록 조정\n","#         x = torch.cat([x3, x_ppm], dim=1)\n","\n","#         # Classifier 수행\n","#         x = self.final(x)\n","\n","#         # 마지막으로 224x224 크기로 다시 조정\n","#         x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=True)\n","\n","#         return x\n","\n","# class PSPModule(nn.Module):\n","#     def __init__(self, in_channels, out_channels, sizes=(1, 2, 3, 6)):\n","#         super(PSPModule, self).__init__()\n","\n","#         self.stages = nn.ModuleList([self._make_stage(in_channels, size) for size in sizes])\n","#         self.bottleneck = nn.Conv2d(in_channels + len(sizes) * out_channels, out_channels, kernel_size=3, padding=1)\n","#         self.relu = nn.ReLU()\n","\n","#     def _make_stage(self, in_channels, size):\n","#         prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n","#         conv = nn.Conv2d(in_channels, 256, kernel_size=1, bias=False)\n","#         relu = nn.ReLU()\n","#         return nn.Sequential(prior, conv, relu)\n","\n","#     def forward(self, x):\n","#         priors = [stage(x) for stage in self.stages] + [x]\n","#         priors = [F.interpolate(prior, size=x.shape[2:], mode='bilinear', align_corners=True) for prior in priors]\n","#         priors = torch.cat(priors, dim=1)\n","#         priors = self.bottleneck(priors)\n","#         return self.relu(priors)\n","\n","# # 모델 초기화\n","# in_channels = 3  # 입력 이미지의 채널 수\n","# num_classes = 13  # 클래스 개수 설정 (배경 클래스 + 객체 클래스)\n","# model = LightweightPSPNet(in_channels, num_classes)\n","\n","# # 입력 이미지 크기 설정 (예: 256x256)\n","# input_size = (256, 256)\n","# x = torch.randn(1, in_channels, *input_size)\n","# output = model(x)\n","# print(\"Output Shape:\", output.shape)\n","\n","# # resnet-18 ; 0.16314\n","# step_size=10에서 Epoch 20, Loss: 0.13495590402812196\n","# step_size=15에서 Epoch 20, Loss: 0.12928759333664092 / Epoch 30, Loss: 0.12090408715648927 -> 0.17987"],"metadata":{"id":"5GlsZSNj0LyS"},"id":"5GlsZSNj0LyS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class LightweightPSPNet(nn.Module):\n","#     def __init__(self, in_channels, num_classes):\n","#         super(LightweightPSPNet, self).__init__()\n","\n","#         # EfficientNet B0 모델 불러오기\n","#         self.backbone = EfficientNet.from_pretrained('efficientnet-b0', in_channels=in_channels)\n","\n","#         # Pyramid Pooling Module 정의\n","#         self.ppm = PSPModule(1280, 256, sizes=(1, 2, 3, 6))  # EfficientNet B0의 출력 채널 수는 1280\n","\n","#         # Classifier\n","#         self.final = nn.Sequential(\n","#             nn.Conv2d(1536, num_classes, kernel_size=1)  # EfficientNet B0의 출력 채널 수는 1536\n","#         )\n","\n","#     def forward(self, x):\n","#         # Backbone 네트워크 수행\n","#         x = self.backbone.extract_features(x)\n","\n","#         # Pyramid Pooling Module 수행\n","#         x_ppm = self.ppm(x)\n","\n","#         # Upsampling 및 병합\n","#         x_ppm = nn.functional.interpolate(x_ppm, size=x.size()[2:], mode='bilinear', align_corners=True)\n","#         x = torch.cat([x, x_ppm], dim=1)\n","\n","#         # Classifier 수행\n","#         x = self.final(x)\n","\n","#         # 마지막으로 224x224 크기로 다시 조정\n","#         x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=True)\n","\n","#         return x\n","\n","# class PSPModule(nn.Module):\n","#     def __init__(self, in_channels, out_channels, sizes=(1, 2, 3, 6)):\n","#         super(PSPModule, self).__init__()\n","\n","#         self.stages = nn.ModuleList([self._make_stage(in_channels, size) for size in sizes])\n","#         self.bottleneck = nn.Conv2d(in_channels + len(sizes) * out_channels, out_channels, kernel_size=3, padding=1)\n","#         self.relu = nn.ReLU()\n","\n","#     def _make_stage(self, in_channels, size):\n","#         prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n","#         conv = nn.Conv2d(in_channels, 256, kernel_size=1, bias=False)\n","#         relu = nn.ReLU()\n","#         return nn.Sequential(prior, conv, relu)\n","\n","#     def forward(self, x):\n","#         priors = [stage(x) for stage in self.stages] + [x]\n","#         priors = [nn.functional.interpolate(prior, size=x.shape[2:], mode='bilinear', align_corners=True) for prior in priors]\n","#         priors = torch.cat(priors, dim=1)\n","#         priors = self.bottleneck(priors)\n","#         return self.relu(priors)\n","\n","# # 모델 초기화\n","# in_channels = 3  # 입력 이미지의 채널 수\n","# num_classes = 13  # 클래스 개수 설정 (배경 클래스 + 객체 클래스)\n","# model = LightweightPSPNet(in_channels, num_classes)\n","\n","# # 입력 이미지 크기 설정 (예: 256x256)\n","# input_size = (256, 256)\n","# x = torch.randn(1, in_channels, *input_size)\n","# output = model(x)\n","# print(\"Output Shape:\", output.shape)\n","\n","# # efficientnet-b0 ; 0.2425057713\n","# # step_size=10 , runtime : 30m\n","# # Epoch 30\n","# # Train Loss: 0.20323605360328287, Train Accuracy: 92.18%\n","# # Validation Loss: 0.44996941884358727, Validation Accuracy: 87.96%"],"metadata":{"id":"fZpcQVtSZqbT"},"id":"fZpcQVtSZqbT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LightweightPSPNet(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(LightweightPSPNet, self).__init__()\n","\n","        # EfficientNet B7 모델 불러오기\n","        self.backbone = EfficientNet.from_pretrained('efficientnet-b7', in_channels=in_channels)\n","\n","        # Pyramid Pooling Module 정의\n","        self.ppm = PSPModule(2560, 256, sizes=(1, 2, 3, 6))  # EfficientNet B7의 출력 채널 수는 2560\n","\n","        # Classifier\n","        self.final = nn.Sequential(\n","            nn.Conv2d(2816, num_classes, kernel_size=1)  # EfficientNet B7의 출력 채널 수는 2816\n","        )\n","\n","    def forward(self, x):\n","        # Backbone 네트워크 수행\n","        x = self.backbone.extract_features(x)\n","\n","        # Pyramid Pooling Module 수행\n","        x_ppm = self.ppm(x)\n","\n","        # Upsampling 및 병합\n","        x_ppm = nn.functional.interpolate(x_ppm, size=x.size()[2:], mode='bilinear', align_corners=True)\n","        x = torch.cat([x, x_ppm], dim=1)\n","\n","        # Classifier 수행\n","        x = self.final(x)\n","\n","        # 마지막으로 224x224 크기로 다시 조정\n","        x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=True)\n","\n","        return x\n","\n","class PSPModule(nn.Module):\n","    def __init__(self, in_channels, out_channels, sizes=(1, 2, 3, 6)):\n","        super(PSPModule, self).__init__()\n","\n","        self.stages = nn.ModuleList([self._make_stage(in_channels, size) for size in sizes])\n","        self.bottleneck = nn.Conv2d(in_channels + len(sizes) * out_channels, out_channels, kernel_size=3, padding=1)\n","        self.relu = nn.ReLU()\n","\n","    def _make_stage(self, in_channels, size):\n","        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n","        conv = nn.Conv2d(in_channels, 256, kernel_size=1, bias=False)\n","        relu = nn.ReLU()\n","        return nn.Sequential(prior, conv, relu)\n","\n","    def forward(self, x):\n","        priors = [stage(x) for stage in self.stages] + [x]\n","        priors = [nn.functional.interpolate(prior, size=x.shape[2:], mode='bilinear', align_corners=True) for prior in priors]\n","        priors = torch.cat(priors, dim=1)\n","        priors = self.bottleneck(priors)\n","        return self.relu(priors)\n","\n","# 모델 초기화\n","in_channels = 3  # 입력 이미지의 채널 수\n","num_classes = 13  # 클래스 개수 설정 (배경 클래스 + 객체 클래스)\n","model = LightweightPSPNet(in_channels, num_classes)\n","\n","# 입력 이미지 크기 설정 (예: 256x256)\n","input_size = (256, 256)\n","x = torch.randn(1, in_channels, *input_size)\n","output = model(x)\n","print(\"Output Shape:\", output.shape)\n","\n","# efficientnet-b7 / Validation Accuracy: 88.90%"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nwzPX09uboaS","outputId":"d0f06629-954f-4f00-c895-0dbd9e6ffac8"},"id":"nwzPX09uboaS","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded pretrained weights for efficientnet-b7\n","Output Shape: torch.Size([1, 13, 224, 224])\n"]}]},{"cell_type":"markdown","metadata":{"id":"a0895765-fba0-4fd9-b955-a6c0e43012e9"},"source":["## Model Train"],"id":"a0895765-fba0-4fd9-b955-a6c0e43012e9"},{"cell_type":"code","source":["# 모델을 저장할 디렉토리 설정\n","save_dir = './saved_models'\n","# os.makedirs(save_dir, exist_ok=True) / 경로 생성"],"metadata":{"id":"90STDAjXA_O6"},"id":"90STDAjXA_O6","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63efb381-98c6-4d9b-a3b6-bd11c7fa8c41"},"outputs":[],"source":["# model 초기화\n","model = LightweightPSPNet(in_channels, num_classes).to(device)\n","\n","# loss function과 optimizer 정의\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# step_size: 학습률을 감소시킬 스텝 크기, gamma: 학습률을 감소시킬 비율\n","step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","\n","train_losses = []  # Train loss 저장\n","train_accuracies = []  # Train accuracy 저장\n","val_losses = []  # Validation loss 저장\n","val_accuracies = []  # Validation accuracy 저장\n","\n","# Early stopping을 위한 변수\n","best_val_accuracy = 0.0\n","patience = 10  # 검증 정확도가 patience 횟수 동안 개선되지 않으면 학습 종료\n","counter = 0\n","\n","# Training loop\n","for epoch in range(30):  # 30 에폭 동안 학습합니다.\n","    model.train()\n","    train_loss = 0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, masks in tqdm(dataloader):\n","        images = images.float().to(device)\n","        masks = masks.long().to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, masks.squeeze(1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","        # 정확도 계산\n","        _, predicted = torch.max(outputs, 1)\n","        total_train += masks.view(-1).size(0)\n","        correct_train += (predicted == masks.squeeze(1)).sum().item()\n","\n","    train_accuracy = correct_train / total_train * 100  # 정확도를 100% 기준으로 계산\n","    train_losses.append(train_loss / len(dataloader))\n","    train_accuracies.append(train_accuracy)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for val_images, val_masks in tqdm(val_dataloader):\n","            val_images = val_images.float().to(device)\n","            val_masks = val_masks.long().to(device)\n","\n","            val_outputs = model(val_images)\n","            val_loss += criterion(val_outputs, val_masks.squeeze(1)).item()\n","\n","            # Validation 정확도 계산\n","            _, val_predicted = torch.max(val_outputs, 1)\n","            total_val += val_masks.view(-1).size(0)\n","            correct_val += (val_predicted == val_masks.squeeze(1)).sum().item()\n","\n","    val_accuracy = correct_val / total_val * 100  # 정확도를 100% 기준으로 계산\n","    val_losses.append(val_loss / len(val_dataloader))\n","    val_accuracies.append(val_accuracy)\n","\n","    step_lr_scheduler.step()\n","\n","    print(f'Epoch {epoch+1}')\n","    print(f'Train Loss: {train_losses[-1]}, Train Accuracy: {train_accuracies[-1]:.2f}%')\n","    print(f'Validation Loss: {val_losses[-1]}, Validation Accuracy: {val_accuracies[-1]:.2f}%')\n","\n","    if (epoch+1) % 5 == 0:\n","      # 그래프 그리기\n","      plt.figure(figsize=(12, 4))\n","\n","      plt.subplot(1, 2, 2)\n","      plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, marker='o', linestyle='-', label='Train Accuracy')\n","      plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, marker='o', linestyle='-', label='Validation Accuracy')\n","      plt.xlabel('Epoch')\n","      plt.ylabel('Accuracy')\n","      plt.title('Training and Validation Accuracy')\n","      plt.grid(True)\n","\n","      plt.show()\n","\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        counter = 0  # 현재 검증 정확도가 더 좋을 경우 counter를 리셋\n","\n","        # 모델 저장\n","        best_model_path = os.path.join(save_dir, 'best_model.pth')\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Saved the best model with validation accuracy: {best_val_accuracy:.2f} in {best_model_path}')\n","\n","    else:\n","        counter += 1\n","\n","    if counter >= patience:\n","        print(f'Early stopping at epoch {epoch+1} as validation accuracy did not improve for {patience} epochs.')\n","        break\n","\n","# 모델 저장\n","        last_model_path = os.path.join(save_dir, 'last_model.pth')\n","        torch.save(model.state_dict(), last_model_path)\n","        print(f'Saved the last model with validation accuracy: {val_accuracy:.2f} in {last_model_path}')\n","\n","# lr=0.001에서 최적\n","\n","# 대략 loss 0.014차이가 score 0.017차이"],"id":"63efb381-98c6-4d9b-a3b6-bd11c7fa8c41"},{"cell_type":"markdown","metadata":{"id":"c32eb51c-a3fe-4e11-a616-3a717ba16f7e"},"source":["## Inference"],"id":"c32eb51c-a3fe-4e11-a616-3a717ba16f7e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"12371c8b-0c78-47df-89ec-2d8b55c8ea94"},"outputs":[],"source":["test_dataset = CustomDataset(csv_file='./test.csv', transform=transform, infer=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"],"id":"12371c8b-0c78-47df-89ec-2d8b55c8ea94"},{"cell_type":"code","source":["# 저장한 모델의 경로 설정\n","model_path = './saved_models/best_model.pth' # 최적의 vaildation accuracy 모델을 사용하려는 경우\n","# model_path = './saved_models/last_model.pth' # 마지막에 학습된 모델을 사용하려는 경우\n","\n","# 모델 초기화\n","model = LightweightPSPNet(in_channels, num_classes).to(device)\n","\n","# 저장한 모델 파라미터 불러오기\n","model.load_state_dict(torch.load(model_path))"],"metadata":{"id":"S1MJKtifCta1"},"id":"S1MJKtifCta1","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"355b431c-ac8e-4c40-9046-4d53e4bab14a"},"outputs":[],"source":["with torch.no_grad():\n","    model.eval()\n","    result = []\n","    for images in tqdm(test_dataloader):\n","        images = images.float().to(device)\n","        outputs = model(images)\n","        outputs = torch.softmax(outputs, dim=1).cpu()\n","        outputs = torch.argmax(outputs, dim=1).numpy()\n","        # batch에 존재하는 각 이미지에 대해서 반복\n","        for pred in outputs:\n","            pred = pred.astype(np.uint8)\n","            pred = Image.fromarray(pred) # 이미지로 변환\n","            pred = pred.resize((960, 540), Image.NEAREST) # 960 x 540 사이즈로 변환\n","            pred = np.array(pred) # 다시 수치로 변환\n","            # class 0 ~ 11에 해당하는 경우에 마스크 형성 / 12(배경)는 제외하고 진행\n","            for class_id in range(12):\n","                class_mask = (pred == class_id).astype(np.uint8)\n","                if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n","                    mask_rle = rle_encode(class_mask)\n","                    result.append(mask_rle)\n","                else: # 마스크가 존재하지 않는 경우 -1\n","                    result.append(-1)"],"id":"355b431c-ac8e-4c40-9046-4d53e4bab14a"},{"cell_type":"markdown","metadata":{"id":"36c2cbbb-04f1-4f9c-b4df-4b744dfce046"},"source":["## Submission"],"id":"36c2cbbb-04f1-4f9c-b4df-4b744dfce046"},{"cell_type":"code","execution_count":null,"metadata":{"id":"35ac2a0b"},"outputs":[],"source":["submit = pd.read_csv('./sample_submission_1.csv')\n","submit['mask_rle'] = result\n","submit"],"id":"35ac2a0b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"da10cb6f-0826-4755-a376-97b695ae8f86"},"outputs":[],"source":["submit.to_csv('./sample_submission_1.csv', index=False)"],"id":"da10cb6f-0826-4755-a376-97b695ae8f86"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}